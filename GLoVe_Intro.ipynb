{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLoVe Embeddings in PyTorch\n",
    "#### Generating Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torchtext.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 100])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this GloVe corpus has been trained using 6 billion words\n",
    "# every word is represented using a dimensionality of 100 and contains 400K words in its vocabulary\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as']\n"
     ]
    }
   ],
   "source": [
    "# Every word in the vocabulary is associated using a unique numeric index \n",
    "# and can be accessed using itos\n",
    "print(glove.itos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 516\n"
     ]
    }
   ],
   "source": [
    "# finding the integer representation of the word\n",
    "# all the words in glove are in lowercase\n",
    "print(glove.stoi['the'], glove.stoi['for'], glove.stoi['london'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 100 dim vector for each words\n",
    "def get_vector(word):\n",
    "    \n",
    "    # will throw assertion error, if word is not present in glove\n",
    "    assert word in glove.stoi, f'*{word}* is not present in the vocab'\n",
    "    \n",
    "    # glove.vectors work with index of the word and not the string val\n",
    "    return glove.vectors[glove.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.0553e-01, -5.0886e-02, -1.5461e-01, -1.2327e-01,  6.6270e-01,\n",
       "        -2.8506e-01, -6.8844e-01,  4.9135e-01, -6.8924e-01,  3.8926e-01,\n",
       "         1.4359e-01, -4.8802e-01,  1.5746e-01,  8.3178e-01, -2.7923e-01,\n",
       "         9.4755e-03, -1.1207e-01, -5.2099e-01, -3.7159e-01, -3.7951e-01,\n",
       "         5.0083e-01, -3.4160e-01,  4.8098e-01, -1.1453e+00,  4.5958e-01,\n",
       "        -6.5640e-01,  4.3018e-01, -4.2527e-01,  2.3089e-01,  7.8911e-01,\n",
       "        -7.5434e-01,  1.0830e-01, -1.8071e-01, -5.5543e-04, -4.1071e-01,\n",
       "         8.6157e-01,  5.3711e-02,  2.4208e-01, -2.6254e-01, -3.0915e-01,\n",
       "        -2.9787e-01, -5.0758e-01, -2.9940e-01, -3.0442e-01,  7.3099e-01,\n",
       "         1.4165e-01,  1.0339e-01, -2.9659e-01,  9.9400e-01, -4.1594e-01,\n",
       "         3.8918e-01,  9.3532e-02,  1.0815e+00,  7.1774e-01, -1.1604e+00,\n",
       "        -3.0277e+00, -9.2490e-01, -8.8455e-02,  6.1408e-01, -2.5770e-01,\n",
       "        -2.6942e-01,  4.4647e-01, -8.3637e-01,  7.2481e-02,  3.0968e-02,\n",
       "        -2.5574e-01, -2.4832e-01,  4.5399e-01,  6.7532e-01, -7.0256e-02,\n",
       "        -1.2594e+00, -3.4572e-01, -4.8970e-01, -5.9699e-01, -1.0352e-01,\n",
       "        -1.2619e+00,  1.2068e+00, -1.4687e-01, -1.2883e+00, -7.5232e-01,\n",
       "         6.5286e-01,  3.8178e-01,  6.6523e-01,  3.1343e-01, -9.5396e-01,\n",
       "        -6.3114e-01, -2.3032e-01, -1.6693e-01, -1.4885e-02,  7.4412e-01,\n",
       "        -1.7945e-01, -5.0046e-01,  8.2233e-01,  2.3110e-01, -7.6910e-01,\n",
       "         9.4608e-01,  4.3809e-01,  4.1467e-01,  3.4641e-01,  1.9886e-02])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector('london')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "*new delhi* is not present in the vocab",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cf0865245202>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# new delhi is not present in the vocab and so it produces an assertion error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new delhi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-f7566f2611f9>\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# will throw assertion error, if word is not present in glove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'*{word}* is not present in the vocab'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# glove.vectors work with index of the word and not the string val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: *new delhi* is not present in the vocab"
     ]
    }
   ],
   "source": [
    "# new delhi is not present in the vocab and so it produces an assertion error\n",
    "get_vector('new delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find similar words similar, first vectorize the input word and then scan through the vocab for similar word vectors\n",
    "# function below returns the closest 6 words of an input word vector\n",
    "\n",
    "def closest_words(vector, n = 6):\n",
    "    distances = []\n",
    "    \n",
    "    # iterate over glove embeddings and calculate dist between input vector and current word\n",
    "    for i in glove.itos:\n",
    "        distances.append((i, torch.dist(vector, get_vector(i))))\n",
    "    \n",
    "    return sorted(distances, key= lambda x: x[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('london', tensor(0.)),\n",
       " ('sydney', tensor(4.2347)),\n",
       " ('paris', tensor(4.6192)),\n",
       " ('melbourne', tensor(4.6299)),\n",
       " ('dublin', tensor(4.6677)),\n",
       " ('edinburgh', tensor(4.8436))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# closest words to London \n",
    "closest_words(get_vector('london'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', tensor(0.)),\n",
       " ('prince', tensor(4.0922)),\n",
       " ('queen', tensor(4.2813)),\n",
       " ('monarch', tensor(4.4742)),\n",
       " ('brother', tensor(4.5367)),\n",
       " ('uncle', tensor(4.6690))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_words(get_vector('king'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating Analogies using glove - king:queen ~ man:?\n",
    "# via equation -> queen - king + man\n",
    "def analogy(w1, w2, w3, n=6):\n",
    "    print('\\n[%s : %s :: %s : ?]' %(w1, w2, w3))\n",
    "    \n",
    "    analogy_vector = get_vector(w2) - get_vector(w1) + get_vector(w3) \n",
    "    closest_analogy = closest_words(analogy_vector, n+3)\n",
    "    \n",
    "    # remove the original words \n",
    "    closest_analogy = [x for x in closest_analogy if x[0] not in (w1, w2, w3)][:n]\n",
    "    \n",
    "    return closest_analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[king : queen :: man : ?]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', tensor(4.0811)),\n",
       " ('girl', tensor(4.6916)),\n",
       " ('she', tensor(5.2703)),\n",
       " ('teenager', tensor(5.2788)),\n",
       " ('boy', tensor(5.3084)),\n",
       " ('mother', tensor(5.3352))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('king','queen','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[sun : hot :: cloud : ?]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sticky', tensor(6.6092)),\n",
       " ('chill', tensor(6.6885)),\n",
       " ('filling', tensor(6.7411)),\n",
       " ('roiling', tensor(6.8482)),\n",
       " ('ash', tensor(6.8523)),\n",
       " ('bubbling', tensor(6.8533))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('sun', 'hot', 'cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[london : england :: paris : ?]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('france', tensor(4.1426)),\n",
       " ('lyon', tensor(5.0363)),\n",
       " ('italy', tensor(5.0874)),\n",
       " ('holland', tensor(5.0943)),\n",
       " ('spain', tensor(5.1148)),\n",
       " ('belgium', tensor(5.1447))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('london', 'england', 'paris')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
